{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training procedure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic training pocedure includes making a word to index mapping based on the occurences of words present in dataset\n",
    ", either go through the dataset and collect occurence of each word in a dictionary , and index them according to their occurences\n",
    ".We can use Tokenizer class from keras library to collect word2index dictionary.\n",
    "\n",
    "Then we make embedding matrix to be used by Embedding layer , we can either randomly initialize it or use already pretrained\n",
    "embeddings on a large dataset such as word2vec ,or glove embeddings ,for this model glove embeddings has been used.See  <a href=\"https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\">this keras blog post</a> for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the multichannel.py from models/ directory , this will provide us with a model , we can initialise it as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open(\"data/pos_polarity.txt\",encoding=\"utf8\")\n",
    "texts=[]\n",
    "labels=[]\n",
    "for line in f:\n",
    "    texts.append(line)\n",
    "    labels.append(1)\n",
    "print(\"found {} texts\".format(len(texts)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open(\"data/neg_polarity.txt\",encoding=\"utf8\")\n",
    "texts_2=[]\n",
    "labels_2=[]\n",
    "for line in f:\n",
    "    texts_2.append(line)\n",
    "    labels_2.append(0)\n",
    "print(\"found {} texts\".format(len(texts_2)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts=texts+texts_2\n",
    "labels=labels+labels_2\n",
    "len(texts),len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_len=[]\n",
    "for sentence in texts:\n",
    "    sentence_len.append(len(sentence))\n",
    "plt.hist(sentence_len,cumulative=True,bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size=15000\n",
    "max_seq_len=250\n",
    "embedding_dim=100\n",
    "validation_split=0.7  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer=Tokenizer(num_words=vocab_size)\n",
    "\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences=tokenizer.texts_to_sequences(texts)\n",
    "data=pad_sequences(sequences,maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare the embedding layer ,using embedding index and\n",
    "embedding_index={}\n",
    "f=open(\"glove.6B.100d.txt\",encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values=line.strip().split()\n",
    "    word=values[0]\n",
    "    \n",
    "    if word  in tokenizer.word_index:\n",
    "        embedding_index[word]=np.asarray(values[1:],dtype=\"float32\")\n",
    "        \n",
    "print(\"embedding index ready\")\n",
    "\n",
    "\n",
    "embedding_matrix=np.zeros((vocab_size+1,embedding_dim))\n",
    " \n",
    "for word,i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        \n",
    "        embedding_matrix[i]=embedding_index[word]\n",
    "    except:\n",
    "        \n",
    "            try:\n",
    "                embedding_matrix[i]=np.random.normal(scale=0.05,size=(embedding_dim,))\n",
    "            except:\n",
    "                print(\"index error\")\n",
    "                \n",
    "                \n",
    "print(\"Embedding matrix ready....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are mainly two models, providing drop argument will also add a dropout layer with p=0.5 after Global max pooling layer ,\n",
    "another is multichannel spatial_dropout which add spatial dropout1d layers.\n",
    " it drops entire 1D feature maps instead of individual elements. If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout1D will help promote independence between feature maps and should be used instead.(Source : keras documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import models.multichannel\n",
    "model=multichannel_spatial.multichannel_drop(nb_classes=1,vocab_size=vocab_size,matrix=embedding_matrix,input_len=max_seq_len,drop=False)\n",
    "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "model.load_weights(\"weights/polarity.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare the data\n",
    "indices=np.random.choice(data.shape[0],replace=False,size=data.shape[0])\n",
    "labels=np.array(labels).reshape(-1,1)\n",
    "num_validation_samples=int(validation_split*data.shape[0])\n",
    "data=data[indices]\n",
    "labels=labels[indices]\n",
    "x_train,y_train=data[:num_validation_samples],labels[:num_validation_samples]\n",
    "x_test,y_test=data[num_validation_samples:],labels[num_validation_samples:]\n",
    "x_train.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train,epochs=1,batch_size=128,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer.word_index,open(\"weights/word2index.p\",\"wb\"))\n",
    "dic=pickle.load(open(\"weights/word2index.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    #preprocess any query to make it model compatible\n",
    "    temp=[]\n",
    "    for word in text.strip().split():\n",
    "        print(word)\n",
    "        if word in word2index: \n",
    "            if word2index[word] < vocab_size:\n",
    "                temp.append(word2index[word])\n",
    "            else:\n",
    "                temp.append(vocab_size+1)\n",
    "        \n",
    "    print(temp)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
